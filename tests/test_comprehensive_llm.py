import os
import sys
import traceback
from unittest.mock import patch, MagicMock

# Flask app context iÃ§in gerekli import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app import create_app
from app.utils.llm_utils import (
    analyze_text_with_llm,
    parse_llm_results,
    DEFAULT_ANALYSIS_CATEGORIES
)

class TestComprehensiveLLM:
    """
    TÃ¼m LLM saÄŸlayÄ±cÄ±larÄ± iÃ§in kapsamlÄ± test sÄ±nÄ±fÄ±
    """
    
    @classmethod
    def setup_class(cls):
        """Test sÄ±nÄ±fÄ± baÅŸlatma"""
        cls.app = create_app()
        cls.app_context = cls.app.app_context()
        cls.app_context.push()
        
        # Test metni - daha zengin iÃ§erik
        cls.test_text = """
        Ã–zet
        Bu projede, Veri TabanÄ± YÃ¶netim Sistemleri dersinde Ã¶ÄŸrenilen bilgilerin pratiÄŸe
        dÃ¶kÃ¼lmesi amacÄ±yla RestauPOS isimli bir restoran otomasyonu geliÅŸtirilmiÅŸtir. Bu 
        otomasyon, restoranlarÄ±n mÃ¼ÅŸteri sipariÅŸlerini, stok takibini, fatura oluÅŸturma 
        iÅŸlemlerini yÃ¶netmeye yÃ¶neliktir.

        Grup Ãœyeleri:
        - Ahmet YÄ±lmaz (Backend kodlama)
        - AyÅŸe Kaya (VeritabanÄ± tasarÄ±mÄ±)
        - Mehmet Demir (ArayÃ¼z tasarÄ±mÄ±)
        - Fatma Ã–zkan (Test uzmanÄ±)

        GÃ¶rev DaÄŸÄ±lÄ±mÄ±:
        Ahmet: Backend kodlama, API entegrasyonu
        AyÅŸe: VeritabanÄ± tasarÄ±mÄ±, SQL sorgularÄ±
        Mehmet: Frontend arayÃ¼zÃ¼, UX/UI tasarÄ±mÄ±
        Fatma: Test senaryolarÄ±, kalite kontrol

        Diyagramlar:
        VeritabanÄ± ER diyagramÄ±: AyÅŸe Kaya tarafÄ±ndan Ã§izilmiÅŸtir.
        KullanÄ±m akÄ±ÅŸ diyagramÄ±: Mehmet Demir tarafÄ±ndan hazÄ±rlanmÄ±ÅŸtÄ±r.
        Sistem mimarisi diyagramÄ±: Ahmet YÄ±lmaz tarafÄ±ndan oluÅŸturulmuÅŸtur.

        BaÅŸlÄ±klar:
        1. GiriÅŸ
        2. Sistem Analizi
        3. Veri TabanÄ± TasarÄ±mÄ±
        4. KullanÄ±cÄ± ArayÃ¼zÃ¼
        5. Test ve Kalite Kontrol
        6. SonuÃ§ ve Ã–neriler

        Eksik KÄ±sÄ±mlar:
        - GÃ¼venlik bÃ¶lÃ¼mÃ¼ eksik
        - Performans analizi yapÄ±lmamÄ±ÅŸ
        - Mobil uyumluluk testi yapÄ±lmamÄ±ÅŸ
        """
        
        # Ã–zel kategoriler test iÃ§in
        cls.custom_categories = {
            "custom_team": "Proje ekibi kimlerden oluÅŸuyor?",
            "custom_technologies": "Hangi teknolojiler kullanÄ±lmÄ±ÅŸ?",
            "custom_challenges": "Projede karÅŸÄ±laÅŸÄ±lan zorluklar neler?"
        }
        
        # Mock LLM yanÄ±tlarÄ± - varsayÄ±lan kategoriler iÃ§in
        cls.mock_default_response = """1. Grup Ã¼yeleri:
        â€¢ Ahmet YÄ±lmaz (Backend kodlama)
        â€¢ AyÅŸe Kaya (VeritabanÄ± tasarÄ±mÄ±)
        â€¢ Mehmet Demir (ArayÃ¼z tasarÄ±mÄ±)
        â€¢ Fatma Ã–zkan (Test uzmanÄ±)

        2. Kim hangi bÃ¶lÃ¼mden sorumlu?
        â€¢ Ahmet: Backend kodlama, API entegrasyonu
        â€¢ AyÅŸe: VeritabanÄ± tasarÄ±mÄ±, SQL sorgularÄ±
        â€¢ Mehmet: Frontend arayÃ¼zÃ¼, UX/UI tasarÄ±mÄ±
        â€¢ Fatma: Test senaryolarÄ±, kalite kontrol

        3. DiyagramlarÄ± kim Ã§izmiÅŸ?
        â€¢ VeritabanÄ± ER diyagramÄ±: AyÅŸe Kaya tarafÄ±ndan Ã§izilmiÅŸtir
        â€¢ KullanÄ±m akÄ±ÅŸ diyagramÄ±: Mehmet Demir tarafÄ±ndan hazÄ±rlanmÄ±ÅŸtÄ±r
        â€¢ Sistem mimarisi diyagramÄ±: Ahmet YÄ±lmaz tarafÄ±ndan oluÅŸturulmuÅŸtur

        4. Belirgin baÅŸlÄ±klar neler?
        â€¢ GiriÅŸ
        â€¢ Sistem Analizi
        â€¢ Veri TabanÄ± TasarÄ±mÄ±
        â€¢ KullanÄ±cÄ± ArayÃ¼zÃ¼
        â€¢ Test ve Kalite Kontrol
        â€¢ SonuÃ§ ve Ã–neriler

        5. Eksik kÄ±sÄ±mlar:
        â€¢ GÃ¼venlik bÃ¶lÃ¼mÃ¼ eksik
        â€¢ Performans analizi yapÄ±lmamÄ±ÅŸ
        â€¢ Mobil uyumluluk testi yapÄ±lmamÄ±ÅŸ"""
        
        # Mock LLM yanÄ±tlarÄ± - Ã¶zel kategoriler iÃ§in
        cls.mock_custom_response = """â€¢ Proje ekibi kimlerden oluÅŸuyor?
        Ahmet YÄ±lmaz, AyÅŸe Kaya, Mehmet Demir ve Fatma Ã–zkan olmak Ã¼zere 4 kiÅŸilik bir ekipten oluÅŸmaktadÄ±r.

        â€¢ Hangi teknolojiler kullanÄ±lmÄ±ÅŸ?
        Veri TabanÄ± YÃ¶netim Sistemleri, Backend kodlama teknolojileri, Frontend arayÃ¼z teknolojileri kullanÄ±lmÄ±ÅŸtÄ±r.

        â€¢ Projede karÅŸÄ±laÅŸÄ±lan zorluklar neler?
        Bu konuda bilgi bulunamadÄ±."""
    
    @classmethod
    def teardown_class(cls):
        """Test sÄ±nÄ±fÄ± sonlandÄ±rma"""
        cls.app_context.pop()
    
    def test_parse_standard_categories_with_numbered_headers(self):
        """Standart kategoriler iÃ§in numaralÄ± baÅŸlÄ±klarÄ± test et"""
        print("\n=== Standart Kategoriler - NumaralÄ± BaÅŸlÄ±k Testi ===")
        
        # Mock analysis object oluÅŸtur
        analysis = {key: [] for key in DEFAULT_ANALYSIS_CATEGORIES.keys()}
        analysis["ham_sonuc"] = []  # Liste olarak baÅŸlat
        
        # Test et
        result = parse_llm_results(
            self.mock_default_response, 
            DEFAULT_ANALYSIS_CATEGORIES
        )
        
        # SonuÃ§larÄ± kontrol et
        assert len(result["grup_uyeleri"]) > 0, "Grup Ã¼yeleri boÅŸ olmamalÄ±"
        assert len(result["sorumluluklar"]) > 0, "Sorumluluklar boÅŸ olmamalÄ±"
        assert len(result["diyagramlar"]) > 0, "Diyagramlar boÅŸ olmamalÄ±"
        assert len(result["basliklar"]) > 0, "BaÅŸlÄ±klar boÅŸ olmamalÄ±"
        assert len(result["eksikler"]) > 0, "Eksikler boÅŸ olmamalÄ±"
        
        print("âœ… Standart kategoriler baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±")
        print(f"Grup Ã¼yeleri: {result['grup_uyeleri']}")
        print(f"Sorumluluklar: {result['sorumluluklar']}")
        print(f"Diyagramlar: {result['diyagramlar']}")
        print(f"BaÅŸlÄ±klar: {result['basliklar']}")
        print(f"Eksikler: {result['eksikler']}")
        
    def test_parse_custom_categories(self):
        """Ã–zel kategoriler iÃ§in ayrÄ±ÅŸtÄ±rma testi"""
        print("\n=== Ã–zel Kategoriler Testi ===")
        
        # Mock analysis object oluÅŸtur
        analysis = {key: [] for key in self.custom_categories.keys()}
        analysis["ham_sonuc"] = []  # Liste olarak baÅŸlat
        
        # Test et
        result = parse_llm_results(
            self.mock_custom_response,
            self.custom_categories
        )
        
        # Ã–zel kategorilerin var olduÄŸunu kontrol et
        for key in self.custom_categories.keys():
            assert key in result, f"Ã–zel kategori {key} sonuÃ§ta bulunmalÄ±"
        
        print("âœ… Ã–zel kategoriler baÅŸarÄ±yla ayrÄ±ÅŸtÄ±rÄ±ldÄ±")
        for key, value in result.items():
            if key.startswith('custom_'):
                print(f"{key}: {value}")
    
    def test_ollama_provider(self):
        """Ollama saÄŸlayÄ±cÄ±sÄ±nÄ± test et"""
        print("\n=== Ollama SaÄŸlayÄ±cÄ± Testi ===")
        
        # Ollama saÄŸlayÄ±cÄ±sÄ±nÄ± ayarla
        original_provider = os.environ.get('LLM_PROVIDER')
        os.environ['LLM_PROVIDER'] = 'ollama'
        
        try:
            with patch('app.utils.llm.get_llm_client') as mock_client:
                # Mock client setup
                mock_instance = MagicMock()
                mock_instance.generate.return_value = self.mock_default_response
                mock_client.return_value = mock_instance
                
                # Test analizi Ã§alÄ±ÅŸtÄ±r
                result = analyze_text_with_llm([self.test_text])
                
                # SonuÃ§larÄ± kontrol et
                assert "grup_uyeleri" in result
                assert "sorumluluklar" in result
                assert "diyagramlar" in result
                assert "basliklar" in result
                assert "eksikler" in result
                
                print("âœ… Ollama saÄŸlayÄ±cÄ±sÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±")
                print(f"Grup Ã¼yeleri sayÄ±sÄ±: {len(result['grup_uyeleri'])}")
                
        except Exception as e:
            print(f"âŒ Ollama testi baÅŸarÄ±sÄ±z: {str(e)}")
            print(traceback.format_exc())
        finally:
            # Orijinal provider'Ä± geri yÃ¼kle
            if original_provider:
                os.environ['LLM_PROVIDER'] = original_provider
            elif 'LLM_PROVIDER' in os.environ:
                del os.environ['LLM_PROVIDER']
    
    def test_lmstudio_provider(self):
        """LM Studio saÄŸlayÄ±cÄ±sÄ±nÄ± test et"""
        print("\n=== LM Studio SaÄŸlayÄ±cÄ± Testi ===")
        
        # LM Studio saÄŸlayÄ±cÄ±sÄ±nÄ± ayarla
        original_provider = os.environ.get('LLM_PROVIDER')
        os.environ['LLM_PROVIDER'] = 'lmstudio'
        
        try:
            with patch('app.utils.llm.get_llm_client') as mock_client:
                # Mock client setup
                mock_instance = MagicMock()
                mock_instance.generate.return_value = self.mock_default_response
                mock_client.return_value = mock_instance
                
                # Test analizi Ã§alÄ±ÅŸtÄ±r
                result = analyze_text_with_llm([self.test_text])
                
                # SonuÃ§larÄ± kontrol et
                assert "grup_uyeleri" in result
                assert "sorumluluklar" in result
                assert "diyagramlar" in result
                assert "basliklar" in result
                assert "eksikler" in result
                
                print("âœ… LM Studio saÄŸlayÄ±cÄ±sÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±")
                print(f"Diyagramlar sayÄ±sÄ±: {len(result['diyagramlar'])}")
                
        except Exception as e:
            print(f"âŒ LM Studio testi baÅŸarÄ±sÄ±z: {str(e)}")
            print(traceback.format_exc())
        finally:
            # Orijinal provider'Ä± geri yÃ¼kle
            if original_provider:
                os.environ['LLM_PROVIDER'] = original_provider
            elif 'LLM_PROVIDER' in os.environ:
                del os.environ['LLM_PROVIDER']
    
    def test_openai_provider(self):
        """OpenAI saÄŸlayÄ±cÄ±sÄ±nÄ± test et"""
        print("\n=== OpenAI SaÄŸlayÄ±cÄ± Testi ===")
        
        # OpenAI saÄŸlayÄ±cÄ±sÄ±nÄ± ayarla
        original_provider = os.environ.get('LLM_PROVIDER')
        os.environ['LLM_PROVIDER'] = 'openai'
        
        try:
            with patch('app.utils.llm.get_llm_client') as mock_client:
                # Mock client setup
                mock_instance = MagicMock()
                mock_instance.generate.return_value = self.mock_default_response
                mock_client.return_value = mock_instance
                
                # Test analizi Ã§alÄ±ÅŸtÄ±r
                result = analyze_text_with_llm([self.test_text])
                
                # SonuÃ§larÄ± kontrol et
                assert "grup_uyeleri" in result
                assert "sorumluluklar" in result
                assert "diyagramlar" in result
                assert "basliklar" in result
                assert "eksikler" in result
                
                print("âœ… OpenAI saÄŸlayÄ±cÄ±sÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±")
                print(f"BaÅŸlÄ±klar sayÄ±sÄ±: {len(result['basliklar'])}")
                
        except Exception as e:
            print(f"âŒ OpenAI testi baÅŸarÄ±sÄ±z: {str(e)}")
            print(traceback.format_exc())
        finally:
            # Orijinal provider'Ä± geri yÃ¼kle
            if original_provider:
                os.environ['LLM_PROVIDER'] = original_provider
            elif 'LLM_PROVIDER' in os.environ:
                del os.environ['LLM_PROVIDER']
    
    def test_gemini_provider(self):
        """Gemini saÄŸlayÄ±cÄ±sÄ±nÄ± test et"""
        print("\n=== Gemini SaÄŸlayÄ±cÄ± Testi ===")
        
        # Gemini saÄŸlayÄ±cÄ±sÄ±nÄ± ayarla
        original_provider = os.environ.get('LLM_PROVIDER')
        os.environ['LLM_PROVIDER'] = 'gemini'
        
        try:
            with patch('app.utils.llm.get_llm_client') as mock_client:
                # Mock client setup
                mock_instance = MagicMock()
                mock_instance.generate.return_value = self.mock_default_response
                mock_client.return_value = mock_instance
                
                # Test analizi Ã§alÄ±ÅŸtÄ±r
                result = analyze_text_with_llm([self.test_text])
                
                # SonuÃ§larÄ± kontrol et
                assert "grup_uyeleri" in result
                assert "sorumluluklar" in result
                assert "diyagramlar" in result
                assert "basliklar" in result
                assert "eksikler" in result
                
                print("âœ… Gemini saÄŸlayÄ±cÄ±sÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±")
                print(f"Eksikler sayÄ±sÄ±: {len(result['eksikler'])}")
                
        except Exception as e:
            print(f"âŒ Gemini testi baÅŸarÄ±sÄ±z: {str(e)}")
            print(traceback.format_exc())
        finally:
            # Orijinal provider'Ä± geri yÃ¼kle
            if original_provider:
                os.environ['LLM_PROVIDER'] = original_provider
            elif 'LLM_PROVIDER' in os.environ:
                del os.environ['LLM_PROVIDER']
    
    '''def test_claude_provider(self):
        """Claude saÄŸlayÄ±cÄ±sÄ±nÄ± test et"""
        print("\n=== Claude SaÄŸlayÄ±cÄ± Testi ===")
        
        # Claude saÄŸlayÄ±cÄ±sÄ±nÄ± ayarla
        original_provider = os.environ.get('LLM_PROVIDER')
        os.environ['LLM_PROVIDER'] = 'claude'
        
        try:
            with patch('app.utils.llm.get_llm_client') as mock_client:
                # Mock client setup
                mock_instance = MagicMock()
                mock_instance.generate.return_value = self.mock_default_response
                mock_client.return_value = mock_instance
                
                # Test analizi Ã§alÄ±ÅŸtÄ±r
                result = analyze_text_with_llm([self.test_text])
                
                # SonuÃ§larÄ± kontrol et
                assert "grup_uyeleri" in result
                assert "sorumluluklar" in result
                assert "diyagramlar" in result
                assert "basliklar" in result
                assert "eksikler" in result
                
                print("âœ… Claude saÄŸlayÄ±cÄ±sÄ± baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±")
                print(f"Sorumluluklar sayÄ±sÄ±: {len(result['sorumluluklar'])}")
                
        except Exception as e:
            print(f"âŒ Claude testi baÅŸarÄ±sÄ±z: {str(e)}")
            print(traceback.format_exc())
        finally:
            # Orijinal provider'Ä± geri yÃ¼kle
            if original_provider:
                os.environ['LLM_PROVIDER'] = original_provider
            elif 'LLM_PROVIDER' in os.environ:
                del os.environ['LLM_PROVIDER']
    '''
    def test_custom_categories_all_providers(self):
        """TÃ¼m saÄŸlayÄ±cÄ±lar iÃ§in Ã¶zel kategoriler testi"""
        print("\n=== TÃ¼m SaÄŸlayÄ±cÄ±lar iÃ§in Ã–zel Kategoriler Testi ===")
        
        providers = ['ollama', 'lmstudio', 'openai', 'gemini']
        original_provider = os.environ.get('LLM_PROVIDER')
        
        for provider in providers:
            print(f"\n--- {provider.upper()} ile Ã¶zel kategoriler testi ---")
            os.environ['LLM_PROVIDER'] = provider
            
            try:
                with patch('app.utils.llm.get_llm_client') as mock_client:
                    # Mock client setup
                    mock_instance = MagicMock()
                    mock_instance.generate.return_value = self.mock_custom_response
                    mock_client.return_value = mock_instance
                    
                    # Ã–zel kategorilerle test analizi Ã§alÄ±ÅŸtÄ±r
                    result = analyze_text_with_llm(
                        [self.test_text], 
                        categories=self.custom_categories
                    )
                    
                    # Ã–zel kategorilerin varlÄ±ÄŸÄ±nÄ± kontrol et
                    for key in self.custom_categories.keys():
                        assert key in result, f"{provider} iÃ§in {key} kategorisi bulunmalÄ±"
                    
                    print(f"âœ… {provider.upper()} Ã¶zel kategoriler testi baÅŸarÄ±lÄ±")
                    
            except Exception as e:
                print(f"âŒ {provider.upper()} Ã¶zel kategoriler testi baÅŸarÄ±sÄ±z: {str(e)}")
        
        # Orijinal provider'Ä± geri yÃ¼kle
        if original_provider:
            os.environ['LLM_PROVIDER'] = original_provider
        elif 'LLM_PROVIDER' in os.environ:
            del os.environ['LLM_PROVIDER']
    
    def test_error_handling(self):
        """Hata yÃ¶netimi testleri"""
        print("\n=== Hata YÃ¶netimi Testleri ===")
        
        # BoÅŸ metin testi
        result_empty = analyze_text_with_llm([])
        assert "grup_uyeleri" in result_empty
        print("âœ… BoÅŸ metin testi baÅŸarÄ±lÄ±")
        
        # None metin testi
        result_none = analyze_text_with_llm([None])
        assert "grup_uyeleri" in result_none
        print("âœ… None metin testi baÅŸarÄ±lÄ±")
        
        # GeÃ§ersiz kategoriler testi
        invalid_categories = {"invalid_key": "Invalid question?"}
        result_invalid = analyze_text_with_llm([self.test_text], categories=invalid_categories)
        assert "invalid_key" in result_invalid
        print("âœ… GeÃ§ersiz kategoriler testi baÅŸarÄ±lÄ±")
        
        # LLM baÄŸlantÄ± hatasÄ± simÃ¼lasyonu
        with patch('app.utils.llm.get_llm_client') as mock_client:
            mock_client.side_effect = Exception("BaÄŸlantÄ± hatasÄ±")
            
            try:
                result_error = analyze_text_with_llm([self.test_text])
                # Hata durumunda bile temel yapÄ± korunmalÄ±
                assert "grup_uyeleri" in result_error
                print("âœ… LLM baÄŸlantÄ± hatasÄ± testi baÅŸarÄ±lÄ±")
            except Exception as e:
                print(f"âš ï¸ LLM baÄŸlantÄ± hatasÄ± testi: {str(e)}")
    
    def test_regex_pattern_resilience(self):
        """Regex pattern dayanÄ±klÄ±lÄ±k testleri"""
        print("\n=== Regex Pattern DayanÄ±klÄ±lÄ±k Testleri ===")
        
        # Test verileri
        test_cases = [
            "1. Grup Ã¼yeleri",
            "2. Kim hangi bÃ¶lÃ¼mden sorumlu?",
            "3. DiyagramlarÄ± kim Ã§izmiÅŸ?",
            "4. Belirgin baÅŸlÄ±klar neler?",
            "5. Eksik kÄ±sÄ±mlar:"
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nTest {i}: {test_case}")
            
            # Test iÃ§in mock yanÄ±t oluÅŸtur
            response = f"{test_case}\n"
            response += "â€¢ Test iÃ§eriÄŸi 1\n"
            response += "â€¢ Test iÃ§eriÄŸi 2\n"
            response += "â€¢ Test iÃ§eriÄŸi 3\n"
            
            # Test et
            result = parse_llm_results(response, DEFAULT_ANALYSIS_CATEGORIES)
            
            # SonuÃ§larÄ± kontrol et
            for category in DEFAULT_ANALYSIS_CATEGORIES.keys():
                if category in result:
                    print(f"{category}: {result[category]}")
            
            print("âœ… Test baÅŸarÄ±lÄ±")

if __name__ == "__main__":
    # Test sÄ±nÄ±fÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
    test_instance = TestComprehensiveLLM()
    test_instance.setup_class()
    
    try:
        # TÃ¼m testleri Ã§alÄ±ÅŸtÄ±r
        test_instance.test_parse_standard_categories_with_numbered_headers()
        test_instance.test_parse_custom_categories()
        test_instance.test_ollama_provider()
        test_instance.test_lmstudio_provider()
        test_instance.test_openai_provider()
        test_instance.test_gemini_provider()
        #test_instance.test_claude_provider()
        test_instance.test_custom_categories_all_providers()
        test_instance.test_error_handling()
        test_instance.test_regex_pattern_resilience()
        
        print("\nğŸ‰ TÃœM TESTLER BAÅARIYLA TAMAMLANDI!")
        
    except Exception as e:
        print(f"\nâŒ Test hatasÄ±: {str(e)}")
        print(traceback.format_exc())
    finally:
        test_instance.teardown_class() 